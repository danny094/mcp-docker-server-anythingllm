# ðŸ§­ Installation Guide â€” MCP Docker Server for AnythingLLM

> âš™ï¸ This project securely connects **AnythingLLM** with **MCP Tools (Model Context Protocol)** through Docker containers.  
> All services run inside an isolated Docker network â€” no `docker.sock`, no root privileges required.

---

## ðŸ“¦ Included in the Project

| Folder / File | Description |
|----------------|-------------|
| ðŸ§  `anythingllm_data` | Data, plugins, and models for AnythingLLM |
| ðŸ” `decision_rules` | Decision agent (work in progress) |
| âš™ï¸ `docker-compose.yml` | Defines and launches all containers |
| ðŸ§© `dummy_MCP` | Demo MCP server for testing |
| ðŸŒ `mcp_hub` | Hub that manages available MCP tools |
| â° `mcp_time` | Example MCP tool for time queries |
| ðŸŒ‰ `mini_bridge` | Connects AnythingLLM â†” MCP Hub |
| ðŸ”„ `n8n_data` | Optional: workflow automation data |
| ðŸ§  `prompt_injector` | Main controller, prompt rules & security layer |

---

## ðŸ§  Installing Ollama (GPU-Enabled)

Ollama is **not installed automatically**. You can run it separately with GPU support:

```bash
docker pull ollama

docker run -d   --gpus all   -v ollama:/root/.ollama   -p 11434:11434   --name ollama   ollama/ollama
```

> âœ… This starts Ollama with GPU acceleration.  
> Check the logs:
> ```bash
> docker logs -f ollama
> ```
> If you see â€œCUDA initializedâ€, GPU mode is active.

---

## ðŸ‹ Container Setup & Launch

Make sure there are no port conflicts.  
You can adjust ports in `docker-compose.yml`.

### ðŸ“‹ Show running containers
```bash
docker ps
```

---

## ðŸš€ Installation & Startup

1ï¸âƒ£ **Clone the repository**
```bash
git clone https://github.com/danny094/mcp-docker-server-anythingllm.git
cd mcp-docker-server-anythingllm
```

2ï¸âƒ£ **Check or create the Docker network**
```bash
docker network ls
```
If your network (e.g. `danny_ai-net`) doesnâ€™t exist:
```bash
docker network create danny_ai-net
```
> If you prefer a custom network name, edit it inside `docker-compose.yml`.

3ï¸âƒ£ **Start the containers**
```bash
docker compose up -d
```
*(Remove `-d` if you want to see live logs)*

---

## ðŸŒ Connecting to AnythingLLM

1ï¸âƒ£ Open AnythingLLM in your browser:
```
http://YOUR_LOCAL_IP:3001
```

2ï¸âƒ£ Select **Local AI**

3ï¸âƒ£ Under *Local AI Base URL*, enter:
```
http://mini-bridge:4100/v1
```

4ï¸âƒ£ You should now see the model  
**deepseek-r1:14b-qwen-distill-q4_K-M**  
â†’ Select & Save.

5ï¸âƒ£ Go to:
```
Settings â†’ Agent Abilities â†’ MCP Servers
```
Make sure it shows **Bridge: ON** âœ…

---

## ðŸ§ª Testing the Setup

Ask in the AnythingLLM chat:
```
Can you tell me the time?
```

âž¡ï¸ In the terminal (prompt-injector or bridge logs), you should see:
```
ðŸ”— MCP call â†’ mcp-time
```

The model automatically decides to use a tool,  
calls the MCP tool **time**, and returns the current time. ðŸ•’

---

## ðŸ§° Useful Docker Commands

| Action | Command |
|--------|----------|
| Stop containers | `docker compose down` |
| Restart | `docker compose up -d` |
| View logs | `docker logs -f prompt-injector` |
| Full cleanup (images & volumes) | `docker compose down -v --rmi all` |

---

## ðŸ”’ Security Tips

- No `docker.sock` mounting â†’ safe, no root-level access  
- Runs inside dedicated network (e.g. `danny_ai-net`)  
- For external access: use a reverse proxy (like Nginx Proxy Manager) + HTTPS  
- Audit logs are stored in `prompt_injector/audit.log`

---

## âœ… Done!

Your full AnythingLLM + MCP stack is now running  
with GPU-accelerated Ollama, secure bridging, and modular tools. ðŸš€

---
> âœ¨ Created by **Danny** â€” a one-man dev who values security, control, and clean architecture.
