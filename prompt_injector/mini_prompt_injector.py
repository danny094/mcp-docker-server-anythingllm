# mini_prompt_injector.py
import json
import logging
import httpx
import os 
from dotenv import load_dotenv

from fastapi import FastAPI, Request
from security_utils import sanitize_input, validate_tool_access, humanize_result, audit_log



logging.basicConfig(level=logging.INFO, format="üß© [%(levelname)s] %(message)s")

app = FastAPI(title="Prompt Injector - Claude Style")

load_dotenv()
# Modell und URLs aus der .env laden
MODEL_NAME = os.getenv("OLLAMA_MODEL", "deepseek-r1:8b")
OLLAMA_URL = os.getenv("OLLAMA_URL", "http://192.168.0.224:11434/api/chat")
MCP_HUB_URL = os.getenv("MCP_HUB_URL", "http://mcp-hub:4400")              # F√ºr Tool-Weiterleitung

# üß† Claude-Style Systemprompt
SYSTEM_PROMPT = """
Du bist ein pr√§ziser KI-Assistent mit Zugriff auf Tools (MCP).
Verhalte dich wie folgt:

1Ô∏è‚É£ Wenn du die Anfrage direkt beantworten kannst (Erkl√§rung, Meinung, Wissen, Smalltalk),
    antworte sofort nat√ºrlich in Textform.
2Ô∏è‚É£ Wenn ein Tool n√∂tig ist (Zeit, Wetter, Dokumente, externe Daten),
    gib nur JSON im Format zur√ºck:
    {"action": "mcp_call", "tool": "<toolname>", "query": "<benutzerfrage>"}
3Ô∏è‚É£ Beantworte keine philosophischen oder offenen Fragen mit Tool-Calls.
4Ô∏è‚É£ Gib keine JSON-Struktur aus, wenn kein Tool gebraucht wird.
"""

# ============================================================
# üß© DeepSeek-Aufruf
# ============================================================
async def ask_deepseek(user_prompt: str):
    payload = {
        "model": MODEL_NAME,
        "messages": [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": user_prompt},
        ],
        "temperature": 0.7,
        "stream": False,
    }

    async with httpx.AsyncClient(timeout=60.0) as client:
        try:
            r = await client.post(OLLAMA_URL, json=payload)
            r.raise_for_status()
            data = r.json()
            message = data.get("message") or data.get("response") or data
            if isinstance(message, dict):
                text = message.get("content", json.dumps(message))
            else:
                text = str(message)
            return text
        except Exception as e:
            logging.error(f"‚ùå DeepSeek Fehler: {e}")
            return f"‚ö†Ô∏è Modellfehler: {e}"


# ============================================================
# üîß Tool-Aufruf via MCP-Hub
# ============================================================
async def call_mcp_tool(tool: str, query: str):
    rpc_payload = {"jsonrpc": "2.0", "id": 1, "method": "query", "params": {"query": query}}
    url = f"{MCP_HUB_URL}/{tool}"

    async with httpx.AsyncClient(timeout=20.0) as client:
        try:
            logging.info(f"üîó MCP-Aufruf ‚Üí {url}")
            r = await client.post(url, json=rpc_payload)
            r.raise_for_status()
            result = r.json()
            content = (
                result.get("result", {}).get("content")
                or result.get("result", {}).get("time")
                or str(result.get("result"))
            )
            return content
        except Exception as e:
            logging.error(f"‚ùå MCP-Aufruf fehlgeschlagen: {e}")
            return f"‚ö†Ô∏è MCP-Fehler: {e}"


# ============================================================
# üí¨ Haupt-Endpunkt
# ============================================================
@app.post("/api/chat")
async def handle_chat(request: Request):
    body = await request.json()
    prompt = body.get("prompt") or body.get("input") or body.get("content", "")
    logging.info(f"üí¨ Eingabe erhalten: {prompt[:120]}")
    
    # üß© --- SECURITY-LAYER ---
    prompt = sanitize_input(prompt)



    # Schritt 1Ô∏è‚É£ ‚Äì DeepSeek befragen
    deepseek_output = await ask_deepseek(prompt)
    if not isinstance(deepseek_output, str):
        deepseek_output = str(deepseek_output)

    # Schritt 2Ô∏è‚É£ ‚Äì Robust pr√ºfen, ob ein JSON-Toolaufruf enthalten ist
    if "{" in deepseek_output and "}" in deepseek_output and '"tool":' in deepseek_output:
        try:
            # JSON-Fragment isolieren (ignoriert Text vor/nach dem JSON)
            start = deepseek_output.find("{")
            end = deepseek_output.rfind("}") + 1
            json_fragment = deepseek_output[start:end]

            decision = json.loads(json_fragment)
            if decision.get("action") == "mcp_call":
                tool = decision.get("tool")
                query = decision.get("query", "")
                #Sicherheitspr√ºfung:
                if not validate_tool_access(decision.get("tool", "")):
                    return {"final": "Tool nicht erlaubt."}
                
                logging.info(f"üß† Tool-Call erkannt ‚Üí {tool}")
                mcp_result = await call_mcp_tool(tool, query)
                
                 # üßæ Audit Logging
                audit_log(prompt, decision, {"result": mcp_result})

                # ‚ú® Ergebnis versch√∂nern (optional)
                pretty = humanize_result({"result": mcp_result})
                return {"final": pretty}
                
                
        except json.JSONDecodeError as e:
            logging.warning(f"‚ö†Ô∏è JSON-Parsing unvollst√§ndig oder fehlerhaft: {e}")
            return {"final": f"‚ö†Ô∏è Unvollst√§ndige JSON-Ausgabe erkannt. Text: {deepseek_output.strip()[:200]}"}
        except Exception as e:
            logging.error(f"‚ùå Tool-Call Fehler: {e}")
            return {"final": f"‚ö†Ô∏è Fehler bei der Tool-Verarbeitung: {e}"}

    # Schritt 3Ô∏è‚É£ ‚Äì Falls kein Tool-Call oder Parsing fehlgeschlagen ‚Üí Textantwort
    logging.info("üó£Ô∏è Direkte Antwort von DeepSeek oder anderem Modell.")
    return {"final": deepseek_output.strip()}


# ============================================================
# üíö Health Endpoint
# ============================================================
@app.get("/health")
async def health():
    return {
        "status": "ok",
        "model": MODEL_NAME,
        "mode": "claude-style",
        "bridge_ready": True,
        "mcp_target": MCP_HUB_URL
    }
